{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Performance Measures.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkYX-dBqJBIE"
      },
      "source": [
        "F1 Score\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjJS74ArJApr"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def dice(im1, im2):\n",
        "    \"\"\"\n",
        "    Computes the Dice coefficient, a measure of set similarity.\n",
        "    Parameters\n",
        "    ----------\n",
        "    im1 : array-like, bool\n",
        "        Any array of arbitrary size. If not boolean, will be converted.\n",
        "    im2 : array-like, bool\n",
        "        Any other array of identical size. If not boolean, will be converted.\n",
        "    Returns\n",
        "    -------\n",
        "    dice : float\n",
        "        Dice coefficient as a float on range [0,1].\n",
        "        Maximum similarity = 1\n",
        "        No similarity = 0\n",
        "        \n",
        "    Notes\n",
        "    -----\n",
        "    The order of inputs for `dice` is irrelevant. The result will be\n",
        "    identical if `im1` and `im2` are switched.\n",
        "    \"\"\"\n",
        "    im1 = np.asarray(im1).astype(np.bool)\n",
        "    im2 = np.asarray(im2).astype(np.bool)\n",
        "\n",
        "    if im1.shape != im2.shape:\n",
        "        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n",
        "\n",
        "    # Compute Dice coefficient\n",
        "    intersection = np.logical_and(im1, im2)\n",
        "\n",
        "    return 2. * intersection.sum() / (im1.sum() + im2.sum() + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvvifdrfJCnc"
      },
      "source": [
        "from statistics import mean\n",
        "dice_scores = []\n",
        "for m in range(4):\n",
        "    diff= dice(Y_test[m],out[m])\n",
        "    score= (diff)*100\n",
        "    dice_scores.append(score)\n",
        "print(\"List of Dice Scores: \",dice_scores)\n",
        "print(\"Mean of Test Images dice :\",mean(dice_scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEeNGoG5JGhM"
      },
      "source": [
        "Jaccard Index\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRFHn4JwJHDk"
      },
      "source": [
        "def jaccard_index(y_true, y_pred):\n",
        "    intersection = (y_true * y_pred).sum()\n",
        "    union = y_true.sum() + y_pred.sum() - intersection\n",
        "    return (intersection + 1e-15) / (union + 1e-15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8eL5xwUJJ8c"
      },
      "source": [
        "from statistics import mean\n",
        "import tensorflow.keras.backend as K\n",
        "jaccard5 = []\n",
        "for m in range(4):\n",
        "    jc=jaccard_index(Y_test[m],out[m])\n",
        "    jaccard5.append(jc)\n",
        "print(\"List of Jaccard Scores: \",jaccard5)\n",
        "print(\"Mean of Test Jaccard Scores :\",mean(jaccard5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji7bZoq9JNdT"
      },
      "source": [
        "Recall\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gsJ9ZeUJOAD"
      },
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "        true_positives = np.sum(np.round(np.clip(y_true.astype(np.bool) * y_pred.astype(np.bool), 0, 1)))\n",
        "        possible_positives = np.sum(np.round(np.clip(y_true.astype(np.bool), 0, 1)))\n",
        "        recall = true_positives / (possible_positives + 0.0001)\n",
        "        return recall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juPXsdheJnFZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH6ritV3JQzt"
      },
      "source": [
        "from statistics import mean\n",
        "import tensorflow.keras.backend as K\n",
        "recall = []\n",
        "for m in range(4):\n",
        "    rc=recall_m(Y_test[m],out[m])\n",
        "    recall.append(rc)\n",
        "print(\"List of recall: \",recall)\n",
        "print(\"Mean of Test recall :\",mean(recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxIVqj-RJUnU"
      },
      "source": [
        "Precision\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjZTlTmqJVTb"
      },
      "source": [
        "def precision_m(y_true, y_pred):\n",
        "        true_positives = np.sum(np.round(np.clip(y_true.astype(np.bool) * y_pred.astype(np.bool), 0, 1)))\n",
        "        predicted_positives = np.sum(np.round(np.clip(y_pred.astype(np.bool), 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + 0.0001)\n",
        "        return precision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3jDQKj0JXpT"
      },
      "source": [
        "from statistics import mean\n",
        "import tensorflow.keras.backend as K\n",
        "precision = []\n",
        "for m in range(4):\n",
        "    pc=precision_m(Y_test[m],out[m])\n",
        "    precision.append(pc)\n",
        "print(\"List of precision: \",precision)\n",
        "print(\"Mean of Test precision :\",mean(precision))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}